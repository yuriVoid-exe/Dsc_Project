# Ollama Model Configuration

# A URL base onde o servidor Ollama está rodando.
tutor.llm.ollama.base-url=http://localhost:11434

# O nome do modelo de LLM que será utilizado.
tutor.llm.ollama.model-name=phi3:mini

# Timeout para evitar que a aplicação trave
# esperando uma resposta que pode demorar.
tutor.llm.ollama.timeout=300

# Hiperparâmetros do Modelo
# Temperature: Controla a criatividade.
# Dando respostas mais factuais e consistentes.
tutor.llm.model.temperature=0.2

qdrant.collection.name=tutor_ingles_br

# O host onde o serviço Qdrant está rodando.
qdrant.host=localhost

# A porta gRPC que a aplicação Java usará para se conectar.
qdrant.port=6334
#
