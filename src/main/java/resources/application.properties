# Ollama Model Configuration

# A URL base onde o servidor Ollama está rodando.
tutor.llm.ollama.base-url=http://localhost:11434

# O nome do modelo de LLM que será utilizado.
tutor.llm.ollama.model-name=llama3.2:1b

# Timeout para evitar que a aplicação trave
# esperando uma resposta que pode demorar.
tutor.llm.ollama.timeout=120

# Hiperparâmetros do Modelo
# Temperature: Controla a criatividade.
# Dando respostas mais factuais e consistentes.
tutor.llm.model.temperature=0.2
